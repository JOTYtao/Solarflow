# @package _global_

# specify here default training configuration
seed: 42
work_dir: ''
model_name: DGDM
# path to folder with data
data_dir: ${work_dir}/data
# use `python run.py debug=true` for easy debugging!
# this will run 1 train, val and test loop with only 1 batch
# equivalent to running `python run.py trainer.fast_dev_run=true`
# (this is placed here just for easier access from command line)
debug: False
mode: train
pretrained_ckpt_path:
# pretty print config at the start of the run using Rich library
print_config: False

# disable python warnings if they annoy you
ignore_warnings: True
trainer:
  _target_: pytorch_lightning.Trainer
  default_root_dir: ${work_dir}/checkpoints/${model_name}
  gradient_clip_val: 1.0
  devices: 2
  accelerator: "gpu"
  max_epochs: 2000
  min_epochs: 1
  max_steps: 200000
  min_steps: 2000
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 2
  enable_checkpointing: True
  enable_progress_bar: True
  log_every_n_steps: 100
  precision: 16
  sync_batchnorm: False
  benchmark: True
  deterministic: False
  fast_dev_run: False
  overfit_batches: 0.0
  enable_model_summary: True
  strategy: "ddp" # for one gpu using auto
model:
  _target_: models.DGDM.determinisitic_pl.determinisitic_pl
  forecast_steps: 8
  lr: 0.001
  visualize: True
  lr_scheduler_mode: "plateau"
  max_epochs: 1000
  save_dir: ${work_dir}/results/${model_name}
datamodule:
  _target_: data.datamodules.SIS_DataModule
  dataset:
    "data_path": ${data_dir}
    "years": {
      "train": [ 2017, 2018, 2019, 2020 ],
      "val": [ 2021 ],
      "test": [ 2022 ]}
    "input_len": 8
    "pred_len": 8
    "stride": 1
    "use_possible_starts": True
  "batch_size": 32
  "num_workers": 10
  "pin_memory": True

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/loss" # name of the logged metric which determines when model is improving
    save_top_k: 3 # save k best models (determined by above metric)
    save_last: True # additionaly always save model from last epoch
    save_weights_only: True # Save only weights and hyperparams, makes smaller and doesn't include callbacks/optimizer/etc. Generally, this should be True, as haven't really been restarting training runs much
    mode: "min" # can be "max" or "min"
    verbose: True
    dirpath: ${work_dir}/checkpoints/${model_name}
    filename: "epoch_{epoch:03d}-val_loss_{val/loss:.4f}"

  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/loss" # name of the logged metric which determines when model is improving
    patience: 10 # how many epochs of not improving until training stops
    mode: "min" # can be "max" or "min"
    min_delta: 1e-3  # minimum change in the monitored metric needed to qualify as an improvement
    verbose: True
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "epoch"

  model_logging:
    _target_: core.callbacks.NeptuneModelLogger
    model_name: ${model_name}
logger:
    # https://neptune.ai

  neptune:
    _target_: neptune.new.integrations.pytorch_lightning.NeptuneLogger
    api_key: "" # api key of neptune
    project: "project name"
    prefix: ""
    name: ""

hydra:
  run:
    dir: ${work_dir}/logs/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${work_dir}/logs/multiruns/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num}

  # you can set here environment variables that are universal for all users
  # for system specific variables (like data paths) it's better to use .env file!
  job:
    env_set:
      EXAMPLE_VAR: "example_value"

paths:
  output_dir: ${work_dir}/outputs/${model_name}
  log_dir: ${work_dir}/logs/${model_name}