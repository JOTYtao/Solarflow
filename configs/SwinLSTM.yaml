# @package _global_

# specify here default training configuration
seed: 42
work_dir: ''
model_name: Swin_LSTM
# path to folder with data
data_dir: ${work_dir}/data
# use `python run.py debug=true` for easy debugging!
# this will run 1 train, val and test loop with only 1 batch
# equivalent to running `python run.py trainer.fast_dev_run=true`
# (this is placed here just for easier access from command line)
debug: False
mode: train
pretrained_ckpt_path: 
print_config: False
ignore_warnings: True
trainer:
  _target_: pytorch_lightning.Trainer
  default_root_dir: ${work_dir}/checkpoints/${model_name}
  gradient_clip_val: 0.5
  devices: 1
  accelerator: "gpu"
  max_epochs: 200
  min_epochs: 1
  max_steps: 200000
  min_steps: 2000
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 2
  enable_checkpointing: True
  enable_progress_bar: True
  log_every_n_steps: 100
  precision: 32
  sync_batchnorm: False
  benchmark: True
  deterministic: False
  fast_dev_run: False
  overfit_batches: 1
  enable_model_summary: True
  strategy: "auto"
model:
  _target_: models.Swin_LSTM_PL.Swin_LSTMpl
  input_channels: 1
  img_size: 128
  patch_size: 2
  window_size: 4  
  out_channels: 1
  forecast_steps: 8
  lr: 0.001
  visualize: False
  save_dir: ${work_dir}/results/${model_name}
datamodule:
  _target_: data.datamodules.SIS_DataModule
  dataset:
    "data_path": ${data_dir}
    "years": {
      "train": [ 2017, 2018, 2019, 2020 ],
      "val": [ 2021 ],
      "test": [ 2022 ]}
    "input_len": 8
    "pred_len": 8
    "stride": 1
    "use_possible_starts": True
  "batch_size": 32
  "num_workers": 10
  "pin_memory": True
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/loss" # name of the logged metric which determines when model is improving
    save_top_k: 5 # save k best models (determined by above metric)
    save_last: True # additionaly always save model from last epoch
    save_weights_only: True # Save only weights and hyperparams, makes smaller and doesn't include callbacks/optimizer/etc. Generally, this should be True, as haven't really been restarting training runs much
    mode: "min" # can be "max" or "min"
    verbose: False
    dirpath: ${work_dir}/checkpoints/${model_name}
    filename: "epoch_{epoch:03d}-val_loss_{val/loss:.4f}"

  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/loss" # name of the logged metric which determines when model is improving
    patience: 10 # how many epochs of not improving until training stops
    mode: "min" # can be "max" or "min"
    min_delta: 1e-4  # minimum change in the monitored metric needed to qualify as an improvement
    verbose: True
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "epoch"

  model_logging:
    _target_: core.callbacks.NeptuneModelLogger
    model_name: ${model_name}
logger:
    # https://neptune.ai

  neptune:
    _target_: neptune.new.integrations.pytorch_lightning.NeptuneLogger
    api_key: "" # api key is loaded from environment variable
    project: ""
    prefix: ""
    name: ""
paths:
  output_dir: ${work_dir}/outputs/${model_name}
  log_dir: ${work_dir}/logs/${model_name}